#!/usr/bin/env python3
"""Fetch all bonded tokens from Odin.fun and update tokens.toml.

Run periodically to keep the shipped token registry current:

    python scripts/update_tokens.py

Odin.fun API notes:
- /v1/tokens supports limit (max 100) and page (1-indexed)
- No server-side bonded filter — must paginate and filter client-side
- Sorting by volume:desc puts bonded tokens first
"""

import datetime
import sys
from pathlib import Path

from curl_cffi import requests

API_BASE = "https://api.odin.fun/v1"
OUTPUT_PATH = Path(__file__).resolve().parent.parent / "src" / "odin_bots" / "tokens.toml"
PAGE_LIMIT = 100
MAX_PAGES = 500


def fetch_bonded_tokens() -> list[dict]:
    """Fetch all bonded tokens from the Odin.fun API."""
    bonded: list[dict] = []
    consecutive_empty = 0
    page = 1

    while page <= MAX_PAGES:
        resp = requests.get(
            f"{API_BASE}/tokens",
            params={"limit": PAGE_LIMIT, "sort": "volume:desc", "page": page},
            impersonate="chrome",
            headers={"Accept": "application/json"},
            timeout=15,
        )
        resp.raise_for_status()
        data = resp.json()
        tokens = data.get("data", [])

        if not tokens:
            break

        page_bonded = [t for t in tokens if t.get("bonded") is True]
        bonded.extend(page_bonded)

        n_bonded = len(page_bonded)
        print(f"  page {page:3d}: {n_bonded:3d} bonded / {len(tokens)} tokens  (total: {len(bonded)})")

        if n_bonded == 0:
            consecutive_empty += 1
            if consecutive_empty >= 2:
                break
        else:
            consecutive_empty = 0

        page += 1

    return bonded


def load_existing_ids() -> set[str]:
    """Load token IDs from the existing tokens.toml (if any)."""
    if not OUTPUT_PATH.exists():
        return set()

    import tomllib
    with open(OUTPUT_PATH, "rb") as f:
        data = tomllib.load(f)
    return set(data.get("tokens", {}).keys())


def write_toml(tokens: list[dict]) -> None:
    """Write tokens to TOML file, sorted by token ID."""
    # Sort by token ID for stable diffs
    tokens.sort(key=lambda t: t["id"])

    today = datetime.date.today().isoformat()

    lines = [
        f"# Bonded tokens on Odin.fun — auto-generated by scripts/update_tokens.py",
        f"# Last updated: {today}",
        f"# Total: {len(tokens)} bonded tokens",
        f"#",
        f"# Token IDs are the primary key (unique on Odin.fun)",
        f"# Names and tickers are NOT unique — always use the token ID",
        f"# Marketcap is a snapshot used for disambiguation (bonded + highest marketcap wins)",
        f"",
    ]

    for t in tokens:
        token_id = t["id"]
        name = t.get("name", "").replace('"', '\\"')
        ticker = t.get("ticker", "").replace('"', '\\"')
        marketcap = t.get("marketcap", 0)

        created_time = t.get("created_time", "")

        lines.append(f"[tokens.{token_id}]")
        lines.append(f'name = "{name}"')
        lines.append(f'ticker = "{ticker}"')
        lines.append(f"marketcap = {marketcap}")
        lines.append(f'marketcap_date = "{today}"')
        lines.append(f'created_time = "{created_time}"')
        lines.append("")

    OUTPUT_PATH.write_text("\n".join(lines))


def main() -> None:
    print(f"Fetching bonded tokens from {API_BASE}...")
    existing_ids = load_existing_ids()

    tokens = fetch_bonded_tokens()
    if not tokens:
        print("ERROR: No bonded tokens found. API may be down.", file=sys.stderr)
        sys.exit(1)

    new_ids = {t["id"] for t in tokens}
    added = new_ids - existing_ids
    removed = existing_ids - new_ids

    write_toml(tokens)

    print(f"\nWrote {len(tokens)} bonded tokens to {OUTPUT_PATH}")
    if existing_ids:
        print(f"  Added:   {len(added)}")
        print(f"  Removed: {len(removed)}")
        if added:
            print(f"  New IDs: {', '.join(sorted(added)[:10])}{'...' if len(added) > 10 else ''}")
    else:
        print("  (first run — no previous file)")


if __name__ == "__main__":
    main()
